
import pyspark

csv = sc.textFile("/user/hadoop/2007.csv")

csv.count()

csv.first()

csv.take(1)

csv.take(2)

csv.take(3)

csvflatMap = csv.flatMap(lambda x: x.split(","))

csvflatMap.count()

for i in csvflatMap.take(29): print(i)

csvtuples = csvflatMap.map(lambda x: (x, 1))

demo = csv.map(lambda x: (x, 1))

demo.count()

demo.first()

csvtuples = csvflatMap.map(lambda x: (x, 1))

csvtuples.count()

wc1 = csvtuples.reduceByKey(lambda x, y: x + y)

help(csvtuples.reduceByKey)

for i in wc1.collect(): print(i)

help(csvtuples.sortBy)

wc1.sortBy(lambda x: x[1]).collect()

wc1.sortBy(lambda x: x[0]).collect()

help(wc1.distinct)

wc1.count()

csvtuples.distinct().collect()

csvtuples.distinct().count()

csvtuples.count()
